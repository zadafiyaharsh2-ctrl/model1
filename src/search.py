import os
from dotenv import load_dotenv
from src.vectorstore import FaissVectorStore
from langchain_groq import ChatGroq

load_dotenv()

class RAGSearch:
    def __init__(self, persist_dir : str = "faiss_store", embedding_model : str = "all-MiniLM-L6-v2", llm_model : str = "llama-3.1-8b-instant"):
        self.vectorstore = FaissVectorStore(persist_dir, embedding_model)
        
        faiss_path = os.path.join(persist_dir, "faiss.index")
        meta_path = os.path.join(persist_dir, "metadata.pkl")
        if not (os.path.exists(faiss_path) and os.path.exists(meta_path)):
            from src.data_loader import load_all_documents
            docs = load_all_documents("Research/data")
            self.vectorstore.build_from_documents(docs)
        else:
            self.vectorstore.load()
        
        groq_api_key = os.getenv("GROQ_API_KEY")
        if not groq_api_key:
            raise ValueError("GROQ_API_KEY missing")
        
        self.llm = ChatGroq(groq_api_key = groq_api_key, model_name = llm_model)
        print(f"[INFO] Groq LLM initialized: {llm_model}")
        
    def search_and_summarize(self, query : str, top_k : int = 5) -> str:
        results = self.vectorstore.query(query, top_k = top_k)
        texts = [r["metadata"]["texts"] for r in results if r.get("metadata")]
        context = "\n\n".join(texts)
        if not context:
            return "No relevant documents found."
        prompt = f"""You are a helpful assistant. Answer the user's query: '{query}'
        
Prioritize Context: Use the provided text as the primary source. If the answer isn't there, use general knowledge but clearly state the source.

Structure: Use a hierarchical format (H2/H3 headers, bullet points, and tables). No walls of text.

Density: Provide a high signal-to-noise ratio. Eliminate filler phrases like "It is important to note" or "I hope this helps."

Formatting: Use bolding for key terms and code blocks for technical syntax.

Tone: Direct, logical, and concise.
Also if the context is from the documents, then mention "THE INFORMATION IS FROM THE DOCUMENTS PROVIDED".
Context:
{context}

Answer:"""
        response = self.llm.invoke(prompt)
        return response.content
    
if __name__ == "__main__":
    rag_search = RAGSearch()
    query = "What is Data Base Management System?"
    summary = rag_search.search_and_summarize(query, top_k=3)
    print("Summary:", summary)